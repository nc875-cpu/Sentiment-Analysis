# -*- coding: utf-8 -*-
"""Untitled90.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12b3aGCI7vAZuKoMh2gdO6AZzn1S4YJQT

# SENTIMENT ANALYSIS OF IPHONE 17 REDDIT POSTS USING NLP

importing libraries
"""

!pip install vaderSentiment

import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import matplotlib.pyplot as plt
import seaborn as sns

nltk.download('stopwords')
nltk.download('wordnet')

"""Loading our dataset"""

url = "https://raw.githubusercontent.com/nc875-cpu/Sentiment-Analysis/main/iphone17_reddit.csv"
df = pd.read_csv(url, encoding='utf-8', on_bad_lines='skip')

print("Dataset shape:", df.shape)
print("Columns:", df.columns)
df.head()

"""##Data cleaning and pre-processing"""

print(df.columns)

# Select the text column manually
df = df[['title']].dropna()
df.rename(columns={'title': 'post'}, inplace=True)

"""detecting text column automatically"""

text_col = [col for col in df.columns if 'text' in col.lower() or 'body' in col.lower() or 'post' in col.lower()]
if len(text_col) == 0:
    raise ValueError("Couldn't detect the text column. Please rename your text column to include 'text' or 'body'.")
df = df[[text_col[0]]].dropna()
df.rename(columns={text_col[0]: 'post'}, inplace=True)

"""text cleaning"""

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)       # removes the URLs
    text = re.sub(r'@\w+|#', '', text)                        # removes the mentions/hashtags
    text = re.sub(r'[^A-Za-z\s]', '', text)                   # removes all the non-letters
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text

df['clean_post'] = df['post'].apply(clean_text)

"""Tokenization, lemmatization and stopwords"""

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]
    return " ".join(words)

df['clean_post'] = df['clean_post'].apply(preprocess_text)

"""Lexicon-Based Sentiment Analysis VADER + TextBlob"""

analyzer = SentimentIntensityAnalyzer()

def vader_sentiment(text):
    score = analyzer.polarity_scores(text)['compound']
    if score >= 0.05:
        return 'positive'
    elif score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

df['vader_sentiment'] = df['clean_post'].apply(vader_sentiment)

def textblob_sentiment(text):
    score = TextBlob(text).sentiment.polarity
    if score > 0:
        return 'positive'
    elif score < 0:
        return 'negative'
    else:
        return 'neutral'

df['textblob_sentiment'] = df['clean_post'].apply(textblob_sentiment)

print("\nLexicon-Based Sentiment Counts (VADER):")
print(df['vader_sentiment'].value_counts())
print("\nLexicon-Based Sentiment Counts (TextBlob):")
print(df['textblob_sentiment'].value_counts())

"""data prep for ML models"""

# Using the results from TextBlob as pseudo labels
df['label'] = df['textblob_sentiment']
df = df[df['label'] != 'neutral']  # keeping only positive/negative values for binary classification

# TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X = vectorizer.fit_transform(df['clean_post'])
y = df['label']

# splitting data for Train-test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Training ML models"""

models = {
    "Naive Bayes": MultinomialNB(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Linear SVM": LinearSVC()
}

results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, pos_label='positive')
    rec = recall_score(y_test, preds, pos_label='positive')
    f1 = f1_score(y_test, preds, pos_label='positive')
    results.append([name, acc, prec, rec, f1])

ml_results = pd.DataFrame(results, columns=["Model", "Accuracy", "Precision", "Recall", "F1-Score"])
print("\nMachine Learning Results:")
print(ml_results)

"""using deep learning LSTM model"""

tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(df['clean_post'])
sequences = tokenizer.texts_to_sequences(df['clean_post'])
padded = pad_sequences(sequences, maxlen=50, padding='post', truncating='post')

label_map = {'negative': 0, 'positive': 1}
df['label_num'] = df['label'].map(label_map)

X_train, X_test, y_train, y_test = train_test_split(padded, df['label_num'], test_size=0.2, random_state=42)

model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=1284, input_length=50))
model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_test, y_test), verbose=1)

loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nDeep Learning (LSTM) Accuracy: {accuracy:.4f}")

"""visualization to compare the accuracies"""

plt.figure(figsize=(8,5))
sns.barplot(x='Model', y='Accuracy', data=ml_results)
plt.title("Machine Learning Model Accuracy Comparison")
plt.show()

"""saving the results"""

df.to_csv("iphone17_sentiment_results.csv", index=False)
print("\nResults saved as iphone17_sentiment_results.csv")

"""summary of insights"""

print("\nSummary of Findings:")
print("- Lexicon-based methods give a quick overview of overall sentiment.")
print("- ML models (Logistic Regression, SVM) provide good accuracy and precision.")
print("- LSTM captures word context and performs best on large text data.")
print("- This helps assess real-time public opinion on iPhone 17 from Reddit posts.")